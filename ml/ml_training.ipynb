{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do\n",
    "- ~~rerun flows from 1981 as start date~~\n",
    "- ~~cast features as category dtype before training catboost~~\n",
    "- ~~engineer categorical features from 'number_of_outings_in_last_year' and 'trip_fishing_effort_hours'~~\n",
    "- ~~re-evaluate models after adding new features -> look at feature importance on catboost model~~\n",
    "- ~~naive bayes outperforming catboost~~\n",
    "- pipreqs to update requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "SCHEMA = 'analytics'\n",
    "DUCKDB_PATH = str(Path().resolve().parent / \"data/noaa_dw.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database Connection and Query\n",
    "with duckdb.connect(DUCKDB_PATH) as con:\n",
    "    query = f\"\"\"\n",
    "            SELECT\n",
    "            fish_caught_time_of_day,\n",
    "            trip_month_name,\n",
    "            fishing_season,\n",
    "            us_region,\n",
    "            nautical_zone,\n",
    "            fishing_method_collapsed,\n",
    "            number_of_outings_in_last_year,\n",
    "            number_of_outings_in_last_2_months,\n",
    "            trip_fishing_effort_hours,\n",
    "            caught\n",
    "            FROM\n",
    "            {SCHEMA}.trip_details\n",
    "            \"\"\"\n",
    "    df = con.sql(query).df() #materialize into pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() #drop any rows that have NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def get_annual_outing_frequency(number_of_annual_outings):\n",
    "    if number_of_annual_outings == 0:\n",
    "        return 'Rarely (First Outing)'\n",
    "    elif number_of_annual_outings <= 50:\n",
    "        return 'Sometimes (<= 50x/year)'\n",
    "    elif number_of_annual_outings <= 100:\n",
    "        return 'Often (50-100x/year)'\n",
    "    else:\n",
    "        return 'Very Often (> 100x/year)'\n",
    "\n",
    "def get_bimonthly_outing_frequency(number_of_bimonthly_outings):\n",
    "    if number_of_bimonthly_outings == 0:\n",
    "        return 'Rarely (First Outing)'\n",
    "    elif number_of_bimonthly_outings <= 5:\n",
    "        return 'Sometimes (<= 5 outings)'\n",
    "    elif number_of_bimonthly_outings <= 15:\n",
    "        return 'Often (5-15 outings)'\n",
    "    else:\n",
    "        return 'Very Often (> 15 outings)'\n",
    "\n",
    "def get_effort_rating(effort_hours):\n",
    "    if effort_hours == 0:\n",
    "        return 'No Effort'\n",
    "    elif effort_hours <= 5:\n",
    "        return 'Low (<= 5 hours)'\n",
    "    elif effort_hours <= 15:\n",
    "        return 'Moderate (5-15 hours)'\n",
    "    else:\n",
    "        return 'High (> 15 hours)'\n",
    "\n",
    "df['annual_outing_freq'] = df['number_of_outings_in_last_year'].apply(get_annual_outing_frequency)\n",
    "df['bimonthly_outing_freq'] = df['number_of_outings_in_last_2_months'].apply(get_bimonthly_outing_frequency)\n",
    "df['effort_rating'] = df['trip_fishing_effort_hours'].apply(get_effort_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame, encode: bool):\n",
    "    #Define Features and Label\n",
    "    drop_cols = ['number_of_outings_in_last_2_months',\n",
    "                'number_of_outings_in_last_year',\n",
    "                'trip_fishing_effort_hours',\n",
    "                'trip_month_name',\n",
    "                'caught']\n",
    "    X = df.drop(columns=drop_cols)\n",
    "    X = X.astype('category') # cast as category dtype\n",
    "\n",
    "    if encode == True:\n",
    "        #Encoding\n",
    "        encoder = OrdinalEncoder()\n",
    "        X = encoder.fit_transform(X)\n",
    "\n",
    "    y = df['caught'].astype(int)\n",
    "\n",
    "    #Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model - Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class =  [0 0 0 ... 1 0 0]\n",
      "proba =  [[0.8608634  0.1391366 ]\n",
      " [0.9300577  0.0699423 ]\n",
      " [0.8803525  0.1196475 ]\n",
      " ...\n",
      " [0.38643774 0.61356226]\n",
      " [0.80022267 0.19977733]\n",
      " [0.73371554 0.26628446]]\n",
      "[[486804 120497]\n",
      " [235797 207249]]\n",
      "0.6501181431045171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73    607301\n",
      "           1       0.63      0.47      0.54    443046\n",
      "\n",
      "    accuracy                           0.66   1050347\n",
      "   macro avg       0.65      0.63      0.63   1050347\n",
      "weighted avg       0.66      0.66      0.65   1050347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train model and predict\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df, encode=True)\n",
    "model = CategoricalNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test) #probability estimates\n",
    "print(\"class = \", y_pred)\n",
    "print(\"proba = \", y_proba)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='weighted'))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 5000 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e58f53a9c894e2a8b8b114a16bcf3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39msample(X_test, sample_size)\n\u001b[1;32m      5\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(model\u001b[38;5;241m.\u001b[39mpredict_proba, X_sample)\n\u001b[0;32m----> 6\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Plot Feature Importance\u001b[39;00m\n\u001b[1;32m      9\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values, X_sample)\n",
      "File \u001b[0;32m~/de_projects/noaa_eda/.venv/lib/python3.11/site-packages/shap/explainers/_kernel.py:271\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[1;32m    270\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[0;32m--> 271\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    273\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/de_projects/noaa_eda/.venv/lib/python3.11/site-packages/shap/explainers/_kernel.py:476\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight_left \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[1;32m    479\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n",
      "File \u001b[0;32m~/de_projects/noaa_eda/.venv/lib/python3.11/site-packages/shap/explainers/_kernel.py:627\u001b[0m, in \u001b[0;36mKernelExplainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m eyVal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN):\n\u001b[0;32m--> 627\u001b[0m     eyVal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN \u001b[38;5;241m+\u001b[39m j, :] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mweights[j]\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mey[i, :] \u001b[38;5;241m=\u001b[39m eyVal\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsamplesRun \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SHAP\n",
    "sample_size = 100\n",
    "X_sample = shap.utils.sample(X_test, sample_size)\n",
    "\n",
    "explainer = shap.KernelExplainer(model.predict_proba, X_sample)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "#Plot Feature Importance\n",
    "shap.summary_plot(shap_values, X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Model - CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6267803\ttotal: 446ms\tremaining: 446ms\n",
      "1:\tlearn: 0.6058692\ttotal: 922ms\tremaining: 0us\n",
      "class =  [0 0 0 ... 0 0 0]\n",
      "proba =  [[0.76362869 0.23637131]\n",
      " [0.84133562 0.15866438]\n",
      " [0.76362869 0.23637131]\n",
      " ...\n",
      " [0.59433522 0.40566478]\n",
      " [0.76362869 0.23637131]\n",
      " [0.76362869 0.23637131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.90      0.76    607301\n",
      "           1       0.73      0.37      0.49    443046\n",
      "\n",
      "    accuracy                           0.68   1050347\n",
      "   macro avg       0.70      0.64      0.63   1050347\n",
      "weighted avg       0.69      0.68      0.65   1050347\n",
      "\n",
      "[[546280  61021]\n",
      " [277284 165762]]\n",
      "0.650256409870547\n",
      "Feature Importance: {'fish_caught_time_of_day': 0.0, 'fishing_season': 0.0, 'us_region': 33.37228722414956, 'nautical_zone': 0.0, 'fishing_method_collapsed': 66.62771277585044, 'annual_outing_freq': 0.0, 'bimonthly_outing_freq': 0.0, 'effort_rating': 0.0}\n",
      "{'learn': {'Logloss': 0.605869243719783}}\n"
     ]
    }
   ],
   "source": [
    "#Train model and predict\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df, encode=False)\n",
    "cat_features = X_train.columns.tolist()\n",
    "model = CatBoostClassifier(iterations=2,\n",
    "                           depth=2,\n",
    "                           learning_rate=1,\n",
    "                           cat_features=cat_features,\n",
    "                           loss_function='Logloss',\n",
    "                           verbose=True)\n",
    "\n",
    "model.fit(X_train, y_train) #fit model\n",
    "y_pred = model.predict(X_test) #predict\n",
    "y_proba = model.predict_proba(X_test) #probability estimates\n",
    "print(\"class = \", y_pred)\n",
    "print(\"proba = \", y_proba)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Feature Importance: {dict(zip(model.feature_names_,model.get_feature_importance()))}')\n",
    "print(f'{model.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values= explainer.shap_values(X_test)\n",
    "\n",
    "# Plot Feature Importance for CatBoost\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
